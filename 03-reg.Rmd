# R code for Regression Analysis {#reg}

This chapter shows how to use **R** for methods covered in a regression analysis course. Some examples follow:

Kutner M.H., Nachtsheim C.J., and Neter, J., *Applied Linear Regression Models*, McGraw-Hill/Irwin.

We refer to this text as ALRM. Datasets are available at:

[https://www.csub.edu/~emontoya2/datasets/textbookdata/index.html](https://www.csub.edu/~emontoya2/datasets/textbookdata/index.html)

## Linear Regression with One Predictor Variable

We illustrate simple linear regression (SLR) using the Toluca Company example from ALRM. The model is:
$$
y_i=\beta_0+\beta_1 x_i + \varepsilon_i  
$$
Here, $y_i$ is the response, $x_i$ the predictor, and errors $\varepsilon_i$ are independent, have mean 0, and constant variance $\sigma^2$.

We use **R** to generate scatterplots, estimate coefficients, obtain fitted values, residuals, and estimate $\sigma^2$.

### Scatterplots

Scatterplots show the relationship between two variables. For bivariate data, the general plotting syntax is `goal(y ~ x, data)`, where `y` is the response and `x` the predictor. To create a scatterplot, use `xyplot`.

Importing the Toluca data:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
require(mosaic)

# Dataset is plain text without variable names, so we customize read.csv
url <- "http://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%201%20Data%20Sets/CH01TA01.txt"
toluca.data <- read.csv(url, header=FALSE, col.names=c("xvar", "yvar"), sep="")
```

Preview the data:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, eval=TRUE, message=FALSE, fig.asp=0.35}
glimpse(toluca.data)
```

Use variable names `yvar` and `xvar` in plotting functions:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
xyplot(yvar ~ xvar, data=toluca.data)
```

Other useful `xyplot()` arguments:
```
xyplot(y ~ x | gfactor, data, main, xlab, ylab, col, pch)
# y: response variable
# x: predictor
# gfactor: optional grouping factor
# data: data frame
# main: plot title
# xlab, ylab: axis labels
# col: point color
# pch: symbol type (1–25)
```


### Least Squares (LS) Estimates

We can compute LS estimates either by applying the algebraic formulas or using the `lm()` function. The general form is `lm(y ~ x, data)`.

Recall the LS estimators:
$$
b_1=\frac{\sum (x_i-\bar{x})(y_i-\bar{y}) }{\sum (x_i-\bar{x})^2}, \quad b_0=\bar{y}- b_1\bar{x}
$$

We apply these in **R**:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
myx <- toluca.data$xvar
myy <- toluca.data$yvar

XminusXbar <- myx - mean(~myx)
YminusYbar <- myy - mean(~myy)
XminusXbarSq <- XminusXbar^2

b.1 <- sum((myx - mean(~myx)) * (myy - mean(~myy))) / sum((myx - mean(~myx))^2)
b.1

b.0 <- mean(~myy) - b.1 * mean(~myx)
b.0
```

Thus, $b_1=3.570$ and $b_0=62.366$.

Alternatively, use `lm()` with the dataframe and variable names:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
lm(yvar ~ xvar, data=toluca.data)
```

To access more model details, save the result to an *lm object*:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
toluca.fit <- lm(yvar ~ xvar, data=toluca.data)
```

Summarize to view coefficients and more:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
summary(toluca.fit)
```

Or use `coef()` to extract just the coefficients:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
coef(toluca.fit)
```

### Plotting the Estimated Regression Line

We can plot the fitted line using either `xyplot()` or `plotModel()`.

#### Using `xyplot()`

Add the estimated line with `type = c("p", "r")`:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
xyplot(yvar ~ xvar, data=toluca.data, type=c("p", "r"))
```

#### Using `plotModel()`

This plots both data and the estimated line:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
plotModel(toluca.fit)
```

To customize aesthetics, use `xyplot()`:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
xyplot(yvar ~ xvar, data=toluca.data, type=c("p", "r"), 
       xlab="Explanatory variable", ylab="Response variable", 
       main="Estimated LS line", col="green", pch=3)
```

### Fitted or Predicted Values

To get fitted values $\hat{y}_i = b_0 + b_1 x_i$, use:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fitted.values(toluca.fit)
predict(toluca.fit)
```

To predict values at new $x$ values (e.g., 31 and 119), use:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
newdat <- data.frame(xvar=c(31, 119))
predict(toluca.fit, newdata=newdat)
```



### Residuals

Residuals are the differences between observed and fitted values: $e_i = y_i - \hat{y}_i$. Use `residuals()` on an *lm object*:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
residuals(toluca.fit)
e <- residuals(toluca.fit)
```

Alternatively, compute them directly:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
yhat <- fitted.values(toluca.fit)
e <- toluca.data$yvar - yhat
```

### `summary()` Output

The `summary()` of an *lm object* includes estimates, standard errors, test statistics, p-values, MSE, $R^2$, and more.

![Figure 3.1](ch3figures/summary.png){ width=90% }

### Estimating $\sigma^2$

Estimate error variance using:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
n <- 25
sum(e^2) / (n - 2)
```

The `summary()` also provides $\hat{\sigma} = \sqrt{MSE}$; here, $\hat{\sigma}^2 = 48.82^2 = 2383.392$.

## Inference in Simple Linear Regression

This section covers inference on regression parameters, mean response, prediction intervals, ANOVA, and association using **R**.

### Inferences for $\beta_0$ and $\beta_1$

We use the Toluca data to perform inference on slope and intercept. Although most attention focuses on $\beta_1$, similar steps apply for $\beta_0$.

Use `summary()` to obtain test statistics and p-values:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
summary(toluca.fit)
```

![](ch3figures/summary.png){ width=90% }

From the output, we get $b_1$ and $b_0$, their standard errors, and t-values.

You can also compute $t = b_1 / s\{b_1\}$ manually:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
MSE <- sum(e^2) / (n - 2)
sd.b1 <- sqrt(MSE / sum((myx - mean(~myx))^2))
t.star <- b.1 / sd.b1
t.star
```

Under $H_0$, the test statistic follows $t_{n-2}$.

To compute p-values:
- For $H_a: \beta > 0$, use:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
xpt(q = 10.290, df = n - 2, lower.tail = FALSE)
```

- For $H_a: \beta < 0$:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
xpt(q = 10.290, df = n - 2, lower.tail = TRUE)
```

- For $H_a: \beta \ne 0$:
$$2 \times P(t_{n-2} > |10.290|) = .0000000004$$

### Confidence Interval for $\beta_1$

The CI formula is:
$$
b_1 \pm t_{1-\alpha/2, n-2} s\{b_1\}
$$

To compute $t_{1 - .05/2, 23}$:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
xqt(1 - .05/2, df = n - 2)
```

Then compute bounds:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
b1 <- 3.570
sb1 <- 0.347

b1 - 2.069 * sb1
b1 + 2.069 * sb1
```

Or use `confint()`:
```{r echo=TRUE, eval=TRUE, message=FALSE, collapse=TRUE, fig.width=5.5, fig.height=3.5, warning=FALSE}
confint(toluca.fit, level = 0.95)
```
```


### CI for the Mean Response ($E(y_h)$) and a Prediction Interval for $y_h$

To compute a CI for $E(y_h)$ at $x_h = 100$, use `predict()` with arguments `interval = "confidence"` and the desired level:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
X.new <- data.frame(xvar = c(100))
predict.lm(toluca.fit, newdata = X.new, interval = "confidence", level = 0.90)
```

To get a prediction interval for a new $y_h$ at $x_h = 100$, change `interval` to `"prediction"`:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
predict.lm(toluca.fit, newdata = X.new, interval = "prediction", level = 0.90)
```

### ANOVA for Regression

Use `anova()` on the *lm object* to get the ANOVA table:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
anova(toluca.fit)
```

![ ](ch3figures/anova.png){ width=90% }

To compute the F-test p-value manually:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
xpf(q = 105.88, df1 = 1, df2 = 25 - 2, lower.tail = FALSE)
```

### Measures of Association

You can compute $R^2$ and $r$ from the ANOVA table or directly:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
yresp <- toluca.data$yvar
xpred <- toluca.data$xvar

SSR <- b.1^2 * sum((xpred - mean(xpred))^2)
SSTO <- sum((yresp - mean(yresp))^2)
R.sq <- SSR / SSTO
R.sq

r <- sqrt(R.sq)  # sign(b.1) is positive
```

### Residual Analysis and Remedial Measures

We assess model assumptions using residual plots. For SLR, if $\varepsilon_i \sim N(0, \sigma^2)$, residuals $e_i$ should reflect this.

| Assumption          | Plot Type                             |
|---------------------|----------------------------------------|
| Normality           | QQ-plot                                |
| Independence        | Residuals vs. $x$ or $\hat{y}$         |
| Constant variance   | Residuals vs. $x$ or $\hat{y}$         |
| Linearity           | Residuals vs. $x$ or $\hat{y}$         |
| Outliers            | Residuals vs. $x$; QQ-plot             |

If normality is not assumed, still check the QQ-plot for heavy tails.

### Creating Residual Plots

Use `xyplot()` and `xqqmath()` for diagnostics:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
tolucafitted <- fitted(toluca.fit)
tolresids <- residuals(toluca.fit)

xyplot(tolresids ~ xvar, data=toluca.data, main="Residuals vs x")
xyplot(tolresids ~ tolucafitted, data=toluca.data, main="Residuals vs fitted")

xqqmath(~ tolresids, distribution = "qnorm")
```

Optional: explore other summaries of residuals:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
gf_boxplot(~ tolresids, main="Boxplot of residuals")
histogram(~ tolresids)
```



### Residual Plots from the `tactile` Package

The `tactile` package extends `xyplot()` to automatically generate residual diagnostics from an *lm object*. For SLR, it includes:

- **Residuals vs. fitted values**: Checks for independence, constant variance, and linearity. No trend or pattern suggests assumptions are met.
- **Normal QQ-plot**: Assesses normality; residuals should align with the diagonal.
- **Standardized residuals vs. fitted values** (scale-location plot): Also checks variance and linearity.
- **Residuals vs. leverage**: Identifies influential points. High leverage + large residuals = outliers.

### Testing for Departures from Normality

Use the Shapiro-Wilk and Lilliefors tests to assess if residuals depart from normality. The null hypothesis is normality.
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
shapiro.test(tolresids)

require("nortest")
lillie.test(tolresids)
```

If p-values are large, there's no strong evidence against normality.

### Transformations

Transforming the response or predictor can help address violations of model assumptions. Use `mutate()` from the `dplyr` package (loaded with `mosaic`) to create transformed variables:

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
glimpse(toluca.data)

# Apply square root to y
toluca.dataVer1 <- mutate(toluca.data, sqrty = sqrt(yvar))
glimpse(toluca.dataVer1)

# Apply log to x
toluca.dataVer2 <- mutate(toluca.dataVer1, lnx = log(xvar))
glimpse(toluca.dataVer2)

# Multiple transformations
toluca.dataWithTrans <- mutate(toluca.data,
  sqrty = sqrt(yvar), lnx = log(xvar),
  lny = log(yvar), sqrd = yvar^2, cubertx = xvar^(1/3))
glimpse(toluca.dataWithTrans)
```

Visualize and fit models using transformed variables:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
xyplot(sqrty ~ xvar, data = toluca.dataWithTrans, main = "Square root of y vs x")
xyplot(sqrty ~ lnx, data = toluca.dataWithTrans, main = "Square root of y vs ln(x)")

toluca.fitA <- lm(sqrty ~ xvar, data = toluca.dataWithTrans)
toluca.fitA
```

### F-Test for Lack of Fit

The lack-of-fit test requires repeated $x$ values. It compares:

- **Full model**: $Y_{ij} = \mu_j + \varepsilon_{ij}$
- **Reduced model**: $Y_{ij} = \beta_0 + \beta_1 X_j + \varepsilon_{ij}$

If models are nested, use `anova()` to compare them:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
url <- "http://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%203%20Data%20Sets/CH03TA04.txt"
bank.data <- read.csv(url, header = FALSE, col.names = c("x", "y"), sep = "")

glimpse(bank.data)

# Convert x to factor for full model
bank.dataUD <- mutate(bank.data, xfac = as.factor(x))

levels.x <- levels(bank.dataUD$xfac)
n <- 11
c <- 6
n - c

fit.bank.full <- lm(y ~ xfac, data = bank.dataUD)
fit.bank.reduced <- lm(y ~ x, data = bank.dataUD)

anova(fit.bank.reduced, fit.bank.full, test = "F")
```

![ ](ch3figures/lof.png){ width=95% }

### Least Absolute Deviation (LAD) Regression

Least squares estimation assumes valid model assumptions. When these fail, least squares can mislead. LAD regression minimizes absolute residuals, making it more robust to outliers.

Diagnostic plots can be helpful but may be inconclusive. Robust regression methods, like LAD, offer alternatives that are less sensitive to violations of assumptions and outliers.



## The Matrix Approach to Simple Linear Regression

### Creating a Matrix

Use `c()` to create vectors:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
xvec <- c(4, 5, 6, 7)
xvec[1]
xvec[3]
```

Create matrices using `matrix()`:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
a <- c(1, 2, 3, 4, 5, 6)
A <- matrix(data = a, nrow = 2, ncol = 3, byrow = FALSE)
B <- matrix(data = a, nrow = 2, ncol = 3, byrow = TRUE)
A
B
```

### Matrix Operations

Key matrix operations in R:

| Operator    | Description                        |
|-------------|------------------------------------|
| `%*%`       | Matrix multiplication              |
| `t()`       | Transpose                          |
| `solve()`   | Inverse of a matrix                |
| `*`         | Element-wise multiplication        |

Examples:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
C <- matrix(c(1, 2, 1, -8, 26, 4, 5, 6, 10), nrow=3)
D <- matrix(c(7, 8, 9, 10, 11, 12, 13, 14, 15), nrow=3)

C %*% D
t(C)
C * D
solve(C)
C + D
```

### LS Estimation

SLR model in matrix form:
$$
\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
$$

Where:
- $\boldsymbol{Y}$: $n \times 1$ response vector
- $\boldsymbol{X}$: $n \times 2$ matrix (intercept and $x$)
- $\boldsymbol{\beta}$: $2 \times 1$ vector ($\beta_0$, $\beta_1$)

Set up the data:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
glimpse(toluca.data)

hrs <- toluca.data$yvar
lotsize <- toluca.data$xvar

yvec <- matrix(hrs, nrow = 25)
ones <- rep(1, 25)
Xmat <- cbind(ones, lotsize)
```

Compute LS estimates:
$$
\boldsymbol{b} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{y}
$$
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
XmatTxmat <- t(Xmat) %*% Xmat
best <- solve(XmatTxmat) %*% t(Xmat) %*% yvec
best
```

Fitted values $\hat{\boldsymbol{y}} = \boldsymbol{X} \boldsymbol{b}$:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
Xmat %*% best
```

Alternatively, use the hat matrix:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
Hatmat <- Xmat %*% solve(XmatTxmat) %*% t(Xmat)
Hatmat %*% yvec
```



## Multiple Linear Regression (MLR)

We consider the MLR model:
$$
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{i,p-1} + \varepsilon_i
$$
where $\varepsilon_i \overset{iid}{\sim} N(0, \sigma^2)$. We illustrate fitting the model using **R** and the Dwaine Studios example from ALRM.

Although the model can be expressed in matrix form, we'll use `lm()` and `anova()` for analysis.

### Least Squares Estimates

Import the data:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
url <- "http://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%206%20Data%20Sets/CH06FI05.txt"
dwaine.data <- read.csv(url, header=FALSE, col.names=c("x1", "x2", "y"), sep="")
glimpse(dwaine.data)
```

Matrix approach:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
n <- 21
Xmat <- cbind(1, dwaine.data$x1, dwaine.data$x2)
yrespvec <- matrix(dwaine.data$y, nrow = n)

b <- solve(t(Xmat) %*% Xmat) %*% t(Xmat) %*% yrespvec
b
```

Fit MLR using `lm()`:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
dwaine.fit <- lm(y ~ x1 + x2, data = dwaine.data)
dwaine.fit
```

### Fitted Values, ANOVA Table, and Residuals

Extract LS estimates:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
coef(dwaine.fit)
```

Get fitted values and residuals:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fitted(dwaine.fit)
residuals(dwaine.fit)
```

Get ANOVA table:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
anova(dwaine.fit)
```

![](ch3figures/anovach6.png){ width=100% }

This table decomposes $SSR(X_1, X_2)$ into $SSR(X_1) + SSR(X_2 \mid X_1)$.

### Inference on Regression Parameters

Summarize the model:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
summary(dwaine.fit)
```

![](ch3figures/summarych6.png){ width=100% }

The output gives $F^*$ for testing $\beta_1 = \beta_2 = 0$ and $t^*$ for individual parameters.

### Mean Response and Prediction Intervals

To get a 95% CI for expected sales when $x_1 = 65.4$ and $x_2 = 17.6$:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
new.x.data <- data.frame(x1 = 65.4, x2 = 17.6)
predict(dwaine.fit, newdata = new.x.data, interval = "confidence", level = 0.95)
```

To get a 95% prediction interval at the same values:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
predict(dwaine.fit, newdata = new.x.data, interval = "prediction", level = 0.95)
```

To predict for two cities:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
new.x.data2 <- data.frame(x1 = c(65.4, 53.1), x2 = c(17.6, 17.7))
predict.lm(dwaine.fit, newdata = new.x.data2, interval = "prediction", level = 0.95)
```



### Diagnostics

Diagnostics in MLR follow the same procedures as in SLR: QQ-plots, residuals vs. fitted values, and residuals vs. predictors. We now also use scatterplot and correlation matrices:

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
splom(dwaine.data)     # Scatterplot matrix
cor(dwaine.data)       # Correlation matrix
```

### ANOVA Table with SSR Decomposition

**R** decomposes SSR based on the order of predictors in `lm()`. For example:
$$
SSR(x_1, x_2, x_3) = SSR(x_3) + SSR(x_2 \mid x_3) + SSR(x_1 \mid x_3, x_2)
$$
To specify this order, list predictors as `x3 + x2 + x1`.

Using the Body Fat example:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, eval=TRUE, message=FALSE, fig.asp=0.35}
url <- "https://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%207%20Data%20Sets/CH07TA01.txt"
bf.data <- read.csv(url, header=FALSE, col.names=c("x1", "x2", "x3", "y"), sep="")
glimpse(bf.data)

bf.fit <- lm(y ~ x3 + x2 + x1, data = bf.data)
anova(bf.fit)
```

This provides the extra sums of squares for $x_2$ and $x_1$ given $x_3$.

### Testing $\beta_k \ne 0$ for Some $k$

To test whether one or more coefficients are significant, compare nested models using `anova()`.

For example, test $H_0: \beta_2 = \beta_3 = 0$ in the Body Fat data:

- **Reduced model**: $y = \beta_0 + \beta_1 x_1 + \varepsilon$
- **Full model**: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon$

Use `anova()` to compare:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, eval=TRUE, message=FALSE, fig.asp=0.35}
reduced.fit <- lm(y ~ x1, data = bf.data)
full.fit <- lm(y ~ x1 + x2 + x3, data = bf.data)

anova(reduced.fit, full.fit, test = "F")
```

![](ch3figures/redTest.png){ width=90% }



## Interactions

To include an interaction between $x_1$ and $x_2$, use `x1*x2` in the model formula. This adds both main effects and the interaction. Alternatively, create the interaction manually with `mutate()`:

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
BodyFat.Inter <- lm(y ~ x1*x2 + x3, data = bf.data)
summary(BodyFat.Inter)

bf.data2 <- mutate(bf.data, x1x2 = x1 * x2)
BodyFat.Inter2 <- lm(y ~ x1 + x2 + x3 + x1x2, data = bf.data2)
summary(BodyFat.Inter2)
```

## Standardized Multiple Regression

To compare coefficient magnitudes, standardize variables using `scale()` within `mutate()`:

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
url <- "http://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%206%20Data%20Sets/CH06FI05.txt"
dwaine.data <- read.csv(url, header = FALSE, col.names = c("x1", "x2", "y"), sep = "")
glimpse(dwaine.data)

dwaine.dataStdz <- mutate(dwaine.data,
  y.stdz = scale(y),
  x1.stdz = scale(x1),
  x2.stdz = scale(x2))

head(dwaine.dataStdz)
```

Fit the model (note: use `-1` to exclude intercept, as variables are standardized):
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
dwaine.fitStdz <- lm(y.stdz ~ x1.stdz + x2.stdz - 1, data = dwaine.dataStdz)
summary(dwaine.fitStdz)
```

The output includes standardized coefficients $b_i^*$.

## Polynomial Regression and Categorical Predictors

We first fit a second-order polynomial model:
$$
y_i = \beta_0 + \beta_1 x_i^* + \beta_2 (x_i^*)^2 + \varepsilon_i
$$
using the `airquality` dataset, where $x_i^* = x_i - \bar{x}$.

Center and square the explanatory variable:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
data(airquality)
glimpse(airquality)

airqualityCen <- mutate(airquality,
  cenx1 = scale(Temp, scale = FALSE),
  cenx1sq = cenx1^2)
```

Fit and plot the model:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
polyfit <- lm(Ozone ~ cenx1 + cenx1sq, data = airqualityCen)
summary(polyfit)

xyplot(Ozone ~ cenx1, data = airqualityCen,
  panel = function(x, y, ...) {
    panel.xyplot(x, y, ...)
    fm <- lm(y ~ poly(x, 2))
    panel.lines(sort(x), predict(fm, newdata = data.frame(x = sort(x))),
                col.line = "black")
  })
```



## Model Selection

We illustrate model selection criteria using the Surgical Unit example from ALRM.

Import the data:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
url <- "http://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%209%20Data%20Sets/CH09TA01.txt"
surgunit.data <- read.csv(url, header=FALSE, col.names=c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "y", "lny"), sep="")
glimpse(surgunit.data)
```

Define categorical variables:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
surgunit.data <- mutate(surgunit.data, x6fac = as.factor(x6), x7fac = as.factor(x7), x8fac = as.factor(x8))
glimpse(surgunit.data)
```

### Adjusted $R^2$ and $C_p$

Adjusted $R^2$ accounts for the number of predictors. To compute it across models, use `leaps`:

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
Xleaps <- select(surgunit.data, -c(lny, y, x6fac, x7fac, x8fac))

R2adj.temp <- leaps(x = Xleaps, y = surgunit.data$lny, method = "adjr2")
R2adj.info <- cbind(R2adj.temp$which, R2adj.temp$size, R2adj.temp$adjr2)
colnames(R2adj.info) <- c("x1","x2","x3","x4","x5","x6","x7","x8","p","adjr2")
R2adj.info
```

For Mallows’ $C_p$:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
Cp.temp <- leaps(x = Xleaps, y = surgunit.data$lny, method = "Cp")
Cp.info <- cbind(Cp.temp$which, Cp.temp$size, Cp.temp$Cp)
colnames(Cp.info) <- c("x1","x2","x3","x4","x5","x6","x7","x8","p","Cp")
Cp.info
```

### AIC

Use `AIC()` on `lm` objects:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fit1 <- lm(lny ~ x1 + x2 + x3 + x5 + x8, data = surgunit.data)
fit2 <- lm(lny ~ x1 + x2 + x3 + x8, data = surgunit.data)
fit3 <- lm(lny ~ x1 + x2 + x4 + x6 + x8, data = surgunit.data)

AIC(fit1, fit2, fit3)
```

### BIC

To compute BIC, add `k = log(n)` to `AIC()`:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
dim(surgunit.data)
AIC(fit1, fit2, fit3, k = log(54))
```

By default, `AIC()` uses `k = 2`.

### AICc

To compute Akaike’s corrected AIC:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
storeAIC <- AIC(fit1, fit2, fit3)

AICc <- storeAIC$AIC + (2 * storeAIC$df^2 + 2 * storeAIC$df) / (54 - storeAIC$df - 1)

storeBIC <- AIC(fit1, fit2, fit3, k = log(54))
cbind(AICc, storeAIC, storeBIC)  # ignore column headers
```



### Stepwise Regression

The `step()` function performs forward/backward stepwise regression using AIC by default. Required arguments include the initial model (`lm` object), `scope` (predictors to consider), and `direction`.

**Forward selection** starts with an intercept-only model and adds predictors that reduce AIC. If no further improvement is possible, it stops.

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fit0 <- lm(lny ~ 1, data = surgunit.data)
fit.fstepAIC <- step(fit0, scope = ~ x1 + x2 + x3 + x4, direction = "forward")
summary(fit.fstepAIC)
```

To use **BIC**:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fit.fstepBIC <- step(fit0, scope = ~ x1 + x2 + x3 + x4, direction = "forward", k = log(54))
summary(fit.fstepBIC)
```

**Backward selection** starts with the full model and removes predictors that reduce AIC.

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fitfull <- lm(lny ~ x1 + x2 + x3 + x4, data = surgunit.data)
fit.BstepAIC <- step(fitfull, scope = ~ x1 + x2 + x3 + x4, direction = "backward")
summary(fit.BstepAIC)
```

With **BIC**:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
fit.BstepBIC <- step(fitfull, scope = ~ x1 + x2 + x3 + x4, direction = "backward", k = log(54))
summary(fit.BstepBIC)
```

---

### CV (PRESS) and GCV

Cross-validation (CV) estimates model predictive accuracy using PRESS. Compute CV scores manually:

```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
n <- 54
cv_score <- function(fit) {
  res <- residuals(fit)
  h <- hatvalues(fit)
  mean((res / (1 - h))^2)
}

CVfit1 <- cv_score(fit1)
CVfit2 <- cv_score(fit2)
CVfit3 <- cv_score(fit3)

CVfit1; CVfit2; CVfit3
```

GCV adjusts for average leverage:
```{r fig.width=7, fig.height=3, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.35}
gcv_score <- function(fit) {
  SSE <- sum(residuals(fit)^2)
  h_bar <- mean(hatvalues(fit))
  SSE / ((1 - h_bar)^2 * n)
}

gcvfit1 <- gcv_score(fit1)
gcvfit2 <- gcv_score(fit2)
gcvfit3 <- gcv_score(fit3)

gcvfit1; gcvfit2; gcvfit3
```

---

## More Diagnostics

### Identifying Outlying y and x Values

The Body Fat dataset illustrates detection of unusual observations via residual diagnostics.

Import data and visualize:
```{r fig.width=7, fig.height=5, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.55}
url <- "https://www.csub.edu/~emontoya2/datasets/textbookdata/Kutner/Chapter%20%207%20Data%20Sets/CH07TA01.txt"
bf.data <- read.csv(url, header=FALSE, col.names=c("x1", "x2", "x3", "y"), sep="")
glimpse(bf.data)
splom(bf.data)
```

Fit model and extract diagnostics:
```{r fig.width=7, fig.height=5, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.55}
fit1 <- lm(y ~ x1 + x2 + x3, data=bf.data)

# Studentized residuals
rstandard(fit1)

# Studentized deleted residuals
rstudent(fit1)

# Leverage (hat values)
hatvalues(fit1)
```

Identify high-leverage points:
```{r fig.width=7, fig.height=5, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.55}
n <- 20; p <- 4
xyplot(hatvalues(fit1) ~ 1:n, panel=function(x, y) {
  panel.xyplot(x, y)
  panel.abline(h = 3 * p / n, col = "red")
}, xlab = "Cases", ylab = "Leverage values", ylim = c(0, 0.7))
```



### Identifying Influential Cases

**DFFITS** measures the influence of each observation on its fitted value. Use `dffits()` with an `lm` object:

```{r fig.width=7, fig.height=5, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.55}
dffits(fit1)

# Threshold: 2*sqrt(p/n), where p = predictors + 1
2 * sqrt((4 + 1) / (20 - 4 - 1))

# Plot DFFITS vs. case number
xyplot(dffits(fit1) ~ 1:20, panel = function(x, y) {
  panel.xyplot(x, y)
  panel.abline(h = 1.154701, col = "red")
}, xlab = "Cases", ylab = "DFFITS", ylim = c(0, 1.2))
```

**Cook's Distance** quantifies influence across all fitted values. Use `cooks.distance()`:

```{r fig.width=7, fig.height=5, fig.align='center', cache=FALSE, echo=TRUE, message=FALSE, fig.asp=0.55}
cooks.distance(fit1)

# Guidelines: D_i > 0.5 may be influential; D_i > 1 likely influential

xyplot(cooks.distance(fit1) ~ 1:20, panel = function(x, y) {
  panel.xyplot(x, y)
  panel.abline(h = 0.5, col = "red")
  panel.abline(h = 1.0, col = "blue")
}, xlab = "Cases", ylab = "Cook's Distance", ylim = c(0, 1.2))
```






